{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(55)\n",
    "np.random.seed(55)\n",
    "\n",
    "input_data = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]  # XOR input\n",
    "output_data = [[0.], [1.], [1.], [0.]]  # XOR output\n",
    "\n",
    "\n",
    "hidden_nodes =2\n",
    "\n",
    "n_input = tf.placeholder(tf.float32, shape=[None, 2], name=\"n_input\")\n",
    "n_output = tf.placeholder(tf.float32, shape=[None, 1], name=\"n_output\")\n",
    "\n",
    "# hidden layer's bias neuron\n",
    "b_hidden = tf.Variable(0.1, name=\"hidden_bias\")  \n",
    "\n",
    "\n",
    "W_hidden = tf.Variable(tf.random_normal([2, hidden_nodes]), name=\"hidden_weights\")\n",
    "\n",
    "hidden = tf.sigmoid(tf.matmul(n_input, W_hidden) + b_hidden)\n",
    "\n",
    "################\n",
    "# output layer #\n",
    "################\n",
    "W_output = tf.Variable(tf.random_normal([hidden_nodes, 1]), name=\"output_weights\")  # output layer's weight matrix\n",
    "\n",
    "\n",
    "#不影响\n",
    "b_output =  tf.Variable(0.1, name=\"output_bias\")\n",
    "\n",
    "\n",
    "\n",
    "output = tf.nn.tanh(tf.matmul(hidden, W_output)+b_output)  # \n",
    "\n",
    "#softmax\n",
    "y = tf.matmul(hidden, W_output)+b_output\n",
    "output = tf.nn.softmax(tf.matmul(hidden, W_output)+b_output)\n",
    "\n",
    "\n",
    "\n",
    "#交叉熵\n",
    "loss = -(n_output * tf.log(output) + (1 - n_output) * tf.log(1 - output))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step:   0\n",
      "loss: [[inf]\n",
      " [nan]\n",
      " [nan]\n",
      " [inf]]\n",
      "\n",
      "step: 200\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 400\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 600\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 800\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 1000\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 1200\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 1400\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 1600\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 1800\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "step: 2000\n",
      "loss: [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "\n",
      "input: [0.0, 0.0] | output: [[nan]]\n",
      "input: [0.0, 1.0] | output: [[nan]]\n",
      "input: [1.0, 0.0] | output: [[nan]]\n",
      "input: [1.0, 1.0] | output: [[nan]]\n"
     ]
    }
   ],
   "source": [
    "train = optimizer.minimize(loss)  # let the optimizer train\n",
    "\n",
    "#####################\n",
    "# train the network #\n",
    "#####################\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(0, 2001):  \n",
    "        # run the training operation\n",
    "        cvalues = sess.run([train, loss, W_hidden, b_hidden, W_output],\n",
    "                       feed_dict={n_input: input_data, n_output: output_data})\n",
    "\n",
    "    # print some debug stuff\n",
    "        if epoch % 200 == 0:\n",
    "            print(\"\")\n",
    "            print(\"step: {:>3}\".format(epoch))\n",
    "            print(\"loss: {}\".format(cvalues[1]))\n",
    "            # print(\"b_hidden: {}\".format(cvalues[3]))\n",
    "            # print(\"W_hidden: {}\".format(cvalues[2]))\n",
    "            # print(\"W_output: {}\".format(cvalues[4]))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"input: {} | output: {}\".format(input_data[0], sess.run(output, feed_dict={n_input: [input_data[0]]})))\n",
    "    print(\"input: {} | output: {}\".format(input_data[1], sess.run(output, feed_dict={n_input: [input_data[1]]})))\n",
    "    print(\"input: {} | output: {}\".format(input_data[2], sess.run(output, feed_dict={n_input: [input_data[2]]})))\n",
    "    print(\"input: {} | output: {}\".format(input_data[3], sess.run(output, feed_dict={n_input: [input_data[3]]})))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
