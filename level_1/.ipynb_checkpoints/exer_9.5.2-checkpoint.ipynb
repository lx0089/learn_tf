{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import ctc_ops\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 自定义\n",
    "yuyinutils = __import__(\"9-24__yuyinutils\")\n",
    "sparse_tuple_to_texts_ch = yuyinutils.sparse_tuple_to_texts_ch\n",
    "ndarray_to_text_ch = yuyinutils.ndarray_to_text_ch\n",
    "get_audio_and_transcriptch = yuyinutils.get_audio_and_transcriptch\n",
    "pad_sequences = yuyinutils.pad_sequences\n",
    "sparse_tuple_from = yuyinutils.sparse_tuple_from\n",
    "get_wavs_lables = yuyinutils.get_wavs_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lx/learn/data_thchs30/train/C17_704.wav 走完 苏 堤 之后 意犹未尽 在 楼外楼 吃 过 西湖 醋 鱼 等 佳肴 之后 又 踏 上了 白 堤\n",
      "wav: 8911 label 8911\n",
      "字表大小: 2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1128 01:17:07.499905 139946114311936 deprecation.py:323] From <ipython-input-3-2e2058e7a663>:180: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1128 01:17:07.526553 139946114311936 deprecation.py:506] From <ipython-input-3-2e2058e7a663>:37: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(8, 615, 494)\n",
      "走完 苏 堤 之后 意犹未尽 在 楼外楼 吃 过 西湖 醋 鱼 等 佳肴 之后 又 踏 上了 白 堤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 01:17:14.049603 139946114311936 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1128 01:17:14.053550 139946114311936 deprecation.py:323] From <ipython-input-3-2e2058e7a663>:57: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1128 01:17:14.074906 139946114311936 deprecation.py:323] From <ipython-input-3-2e2058e7a663>:73: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W1128 01:17:14.076570 139946114311936 deprecation.py:323] From /home/lx/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W1128 01:17:14.195541 139946114311936 deprecation.py:506] From /home/lx/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1128 01:17:14.278519 139946114311936 deprecation.py:506] From /home/lx/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1128 01:17:14.408339 139946114311936 ag_logging.py:145] Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4733dcbfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4733dcbfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1128 01:17:14.423256 139946114311936 deprecation.py:323] From /home/lx/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4733dcbfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4733dcbfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 01:17:14.643588 139946114311936 ag_logging.py:145] Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f47322d66a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f47322d66a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f47322d66a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f47322d66a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "b_stddev = 0.046875\n",
    "h_stddev = 0.046875\n",
    "\n",
    "n_hidden = 1024\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 =1024\n",
    "n_hidden_5 = 1024\n",
    "n_cell_dim = 1024\n",
    "n_hidden_3 = 2 * 1024\n",
    "\n",
    "keep_dropout_rate=0.95\n",
    "relu_clip = 20\n",
    "\n",
    "\n",
    "def BiRNN_model( batch_x, seq_length, n_input, n_context,n_character ,keep_dropout):\n",
    "\n",
    "    # batch_x_shape: [batch_size, n_steps, n_input + 2*n_input*n_context]\n",
    "    batch_x_shape = tf.shape(batch_x)\n",
    "\n",
    "   \n",
    "    # 将输入转成时间序列优先\n",
    "    batch_x = tf.transpose(batch_x, [1, 0, 2])\n",
    "    # 再转成2维传入第一层\n",
    "    batch_x = tf.reshape(batch_x,\n",
    "                         [-1, n_input + 2 * n_input * n_context])  # (n_steps*batch_size, n_input + 2*n_input*n_context)\n",
    "\n",
    "    # 使用clipped RELU activation and dropout.\n",
    "    # 1st layer\n",
    "    with tf.name_scope('fc1'):\n",
    "        b1 = variable_on_cpu('b1', [n_hidden_1], tf.random_normal_initializer(stddev=b_stddev))\n",
    "        h1 = variable_on_cpu('h1', [n_input + 2 * n_input * n_context, n_hidden_1],\n",
    "                             tf.random_normal_initializer(stddev=h_stddev))\n",
    "        layer_1 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(batch_x, h1), b1)), relu_clip)\n",
    "        layer_1 = tf.nn.dropout(layer_1, keep_dropout)\n",
    "\n",
    "    # 2nd layer\n",
    "    with tf.name_scope('fc2'):\n",
    "        b2 = variable_on_cpu('b2', [n_hidden_2], tf.random_normal_initializer(stddev=b_stddev))\n",
    "        h2 = variable_on_cpu('h2', [n_hidden_1, n_hidden_2], tf.random_normal_initializer(stddev=h_stddev))\n",
    "        layer_2 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_1, h2), b2)), relu_clip)\n",
    "        layer_2 = tf.nn.dropout(layer_2, keep_dropout)\n",
    "\n",
    "\n",
    "    # 3rd layer\n",
    "    with tf.name_scope('fc3'):\n",
    "        b3 = variable_on_cpu('b3', [n_hidden_3], tf.random_normal_initializer(stddev=b_stddev))\n",
    "        h3 = variable_on_cpu('h3', [n_hidden_2, n_hidden_3], tf.random_normal_initializer(stddev=h_stddev))\n",
    "        layer_3 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(layer_2, h3), b3)), relu_clip)\n",
    "        layer_3 = tf.nn.dropout(layer_3, keep_dropout)\n",
    "\n",
    "    # 双向rnn\n",
    "    with tf.name_scope('lstm'):\n",
    "        # Forward direction cell:\n",
    "        lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell,\n",
    "                                                     input_keep_prob=keep_dropout)\n",
    "        # Backward direction cell:\n",
    "        lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell,\n",
    "                                                     input_keep_prob=keep_dropout)\n",
    "\n",
    "        # `layer_3`  `[n_steps, batch_size, 2*n_cell_dim]`\n",
    "        layer_3 = tf.reshape(layer_3, [-1, batch_x_shape[0], n_hidden_3])\n",
    "\n",
    "        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                                 cell_bw=lstm_bw_cell,\n",
    "                                                                 inputs=layer_3,\n",
    "                                                                 dtype=tf.float32,\n",
    "                                                                 time_major=True,\n",
    "                                                                 sequence_length=seq_length)\n",
    "\n",
    "        # 连接正反向结果[n_steps, batch_size, 2*n_cell_dim]\n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        # to a single tensor of shape [n_steps*batch_size, 2*n_cell_dim]        \n",
    "        outputs = tf.reshape(outputs, [-1, 2 * n_cell_dim])\n",
    "\n",
    "    with tf.name_scope('fc5'):\n",
    "        b5 = variable_on_cpu('b5', [n_hidden_5], tf.random_normal_initializer(stddev=b_stddev))\n",
    "        h5 = variable_on_cpu('h5', [(2 * n_cell_dim), n_hidden_5], tf.random_normal_initializer(stddev=h_stddev))\n",
    "        layer_5 = tf.minimum(tf.nn.relu(tf.add(tf.matmul(outputs, h5), b5)), relu_clip)\n",
    "        layer_5 = tf.nn.dropout(layer_5, keep_dropout)\n",
    "\n",
    "    with tf.name_scope('fc6'):\n",
    "        # 全连接层用于softmax分类\n",
    "        b6 = variable_on_cpu('b6', [n_character], tf.random_normal_initializer(stddev=b_stddev))\n",
    "        h6 = variable_on_cpu('h6', [n_hidden_5, n_character], tf.random_normal_initializer(stddev=h_stddev))\n",
    "        layer_6 = tf.add(tf.matmul(layer_5, h6), b6)\n",
    "\n",
    "\n",
    "    # 将2维[n_steps*batch_size, n_character]转成3维 time-major [n_steps, batch_size, n_character].\n",
    "    layer_6 = tf.reshape(layer_6, [-1, batch_x_shape[0], n_character])\n",
    "\n",
    "    # Output shape: [n_steps, batch_size, n_character]\n",
    "    return layer_6\n",
    "\n",
    "\"\"\"\n",
    "used to create a variable in CPU memory.\n",
    "\"\"\"    \n",
    "def variable_on_cpu(name, shape, initializer):\n",
    "    # Use the /cpu:0 device for scoped operations\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Create or get apropos variable\n",
    "        var = tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "    return var\n",
    "    \n",
    "\n",
    "wav_path='/home/lx/learn/data_thchs30/train'\n",
    "label_file='/home/lx/learn/doc/trans/train.word.txt'\n",
    "   \n",
    "wav_files, labels = get_wavs_lables(wav_path,label_file) \n",
    "print(wav_files[0], labels[0])  \n",
    "# wav/train/A11/A11_0.WAV -> 绿 是 阳春 烟 景 大块 文章 的 底色 四月 的 林 峦 更是 绿 得 鲜活 秀媚 诗意 盎然  \n",
    "\n",
    "print(\"wav:\",len(wav_files),\"label\",len(labels))\n",
    "\n",
    "\n",
    "# 字表 \n",
    "all_words = []  \n",
    "for label in labels:  \n",
    "    #print(label)    \n",
    "    all_words += [word for word in label]  \n",
    "counter = Counter(all_words)  \n",
    "words = sorted(counter)\n",
    "words_size= len(words)\n",
    "word_num_map = dict(zip(words, range(words_size))) \n",
    "\n",
    "print('字表大小:', words_size) \n",
    " \n",
    "\n",
    "n_input = 26#计算美尔倒谱系数的个数\n",
    "n_context = 9#对于每个时间点，要包含上下文样本的个数\n",
    "batch_size =8\n",
    "def next_batch(labels, start_idx = 0,batch_size=1,wav_files = wav_files):\n",
    "    filesize = len(labels)\n",
    "    end_idx = min(filesize, start_idx + batch_size)\n",
    "    idx_list = range(start_idx, end_idx)\n",
    "    txt_labels = [labels[i] for i in idx_list]\n",
    "    wav_files = [wav_files[i] for i in idx_list]\n",
    "    (source, audio_len, target, transcript_len) = get_audio_and_transcriptch(None,\n",
    "                                                      wav_files,\n",
    "                                                      n_input,\n",
    "                                                      n_context,word_num_map,txt_labels)\n",
    "    \n",
    "    start_idx += batch_size\n",
    "    # Verify that the start_idx is not larger than total available sample size\n",
    "    if start_idx >= filesize:\n",
    "        start_idx = -1\n",
    "\n",
    "    # Pad input to max_time_step of this batch\n",
    "    source, source_lengths = pad_sequences(source)#如果多个文件将长度统一，支持按最大截断或补0\n",
    "    sparse_labels = sparse_tuple_from(target)\n",
    "\n",
    "    return start_idx,source, source_lengths, sparse_labels\n",
    "\n",
    "next_idx,source,source_len,sparse_lab = next_batch(labels,0,batch_size)\n",
    "print(len(sparse_lab))\n",
    "print(np.shape(source))\n",
    "#print(sparse_lab)\n",
    "t = sparse_tuple_to_texts_ch(sparse_lab,words)\n",
    "print(t[0])\n",
    "#source已经将变为前9（不够补空）+本身+后9，每个26，第一个顺序是第10个的数据。\n",
    "\n",
    "\n",
    "\n",
    "# shape = [batch_size, max_stepsize, n_input + (2 * n_input * n_context)]\n",
    "# the batch_size and max_stepsize每步都是变长的。\n",
    "input_tensor = tf.placeholder(tf.float32, [None, None, n_input + (2 * n_input * n_context)], name='input')#语音log filter bank or MFCC features\n",
    "# Use sparse_placeholder; will generate a SparseTensor, required by ctc_loss op.\n",
    "targets = tf.sparse_placeholder(tf.int32, name='targets')#文本\n",
    "# 1d array of size [batch_size]\n",
    "seq_length = tf.placeholder(tf.int32, [None], name='seq_length')#序列长\n",
    "keep_dropout= tf.placeholder(tf.float32)\n",
    "\n",
    "# logits is the non-normalized output/activations from the last layer.\n",
    "# logits will be input for the loss function.\n",
    "# nn_model is from the import statement in the load_model function\n",
    "logits = BiRNN_model( input_tensor, tf.to_int64(seq_length), n_input, n_context,words_size +1,keep_dropout)\n",
    "\n",
    "\n",
    "\n",
    "#调用ctc loss\n",
    "avg_loss = tf.reduce_mean(ctc_ops.ctc_loss(targets, logits, seq_length))\n",
    "\n",
    "\n",
    "#[optimizer]\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(avg_loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"decode\"):    \n",
    "    decoded, log_prob = ctc_ops.ctc_beam_search_decoder( logits, seq_length, merge_repeated=False)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    distance = tf.edit_distance( tf.cast(decoded[0], tf.int32), targets)\n",
    "    # 计算label error rate (accuracy)\n",
    "    ler = tf.reduce_mean(distance, name='label_error_rate')\n",
    "   \n",
    "\n",
    "epochs = 100\n",
    "savedir = \"log/yuyinchalltest/\"\n",
    "saver = tf.train.Saver(max_to_keep=1) # 生成saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kpt: None\n",
      "\n",
      "===========Run training epoch===========\n",
      "\n",
      "epoch start: 0 total epochs=  100\n",
      "total loop  1114 in one epoch， 8 items in one loop\n",
      "loop: 19 Train cost:  655.7648559570313\n",
      "Label err rate:  0.94439113\n",
      " file 0\n",
      "Original: 而 三菱 银行 和 东京 银行 的 合并 原因 则 在于 三菱 银行 看中 了 东京 银行 的 六十 六家 海外 分行\n",
      "Decoded:   的 的 \n",
      "loop: 39 Train cost:  472.1804557800293\n",
      "Label err rate:  0.98130333\n",
      " file 0\n",
      "Original: 铁路 要 从 这里 铺 通 他 不准 挖掘 老 将军 宋 承 福 带头 一 锨 首先 挖 开 了 自己 亲人 的 墓穴\n",
      "Decoded:   \n",
      "loop: 59 Train cost:  405.19721094767254\n",
      "Label err rate:  0.95162225\n",
      " file 0\n",
      "Original: 杨 队长 家 的 二 娃子 发烧 到 区 上 诊所 看了 病 带回 一 包 针药 要 亦 琼 给 他 注射\n",
      "Decoded:   的 \n",
      "loop: 79 Train cost:  369.75858268737795\n",
      "Label err rate:  0.9768586\n",
      " file 0\n",
      "Original: 从 飞 机上 往下 望去 就 如 同一个 硕大 的 玉 盆 一样 盆 底 下面 还有 缕缕 青烟 冒出\n",
      "Decoded:   的 \n",
      "loop: 99 Train cost:  348.9950567626953\n",
      "Label err rate:  0.9638306\n",
      " file 0\n",
      "Original: 秦 华 孙 指出 为了 实现 印度洋 和平区 的 目标 需要 区域 内部 和 外部 国家 的 共同努力\n",
      "Decoded:   龚龚\n",
      "loop: 119 Train cost:  335.8096533457438\n",
      "Label err rate:  0.9647637\n",
      " file 0\n",
      "Original: 这种 疫苗 能够 预防 养殖 的 大马哈鱼 和 鳟鱼 的 疖 病 疫苗 的 使用 可以 大大 减少 水产 养殖业 中 所 广泛 使用 的 抗菌素\n",
      "Decoded:   的 \n",
      "loop: 139 Train cost:  324.80217361450195\n",
      "Label err rate:  0.97700155\n",
      " file 0\n",
      "Original: 猛 禽类 的 猫头鹰 秃鹫 等 脚 强壮 而 有力 趾 端 有 锐 而 钩 曲 的 爪 利于 捕杀 动物\n",
      "Decoded:   的 \n",
      "loop: 159 Train cost:  317.052331829071\n",
      "Label err rate:  0.9716753\n",
      " file 0\n",
      "Original: 中国 要求 在 别国 遗弃 化学武器 的 国家 应 按 公约 规定 尽快 彻底 销毁 所有 遗弃 化学武器\n",
      "Decoded:   龚龚\n",
      "loop: 179 Train cost:  311.6009364657932\n",
      "Label err rate:  0.95577085\n",
      " file 0\n",
      "Original: 图 为 沈阳 旅客 张慧 泉 夫妇 怀抱 二个 可爱的 婴儿 专程 来到 济南 火车站 值班室 感谢 救命恩人\n",
      "Decoded:   的 \n",
      "loop: 199 Train cost:  307.03275344848635\n",
      "Label err rate:  0.95182574\n",
      " file 0\n",
      "Original: 在 黔 阳县 农民 全国 劳模 钦 万 有的 园艺 场 看到 四千 亩 柑桔 林 在 艳阳 下 绿 枝 摇曳 徐 部长 赞不绝口\n",
      "Decoded:   的 \n",
      "loop: 219 Train cost:  303.1029545177113\n",
      "Label err rate:  0.9589139\n",
      " file 0\n",
      "Original: 要知道 急性 心肌梗塞 是 必须 住院治疗 的 到 医院 越早越好 越 晚 死亡率 越 高\n",
      "Decoded:   的 \n",
      "loop: 239 Train cost:  299.84339739481607\n",
      "Label err rate:  0.972242\n",
      " file 0\n",
      "Original: 新 聘 国务院 参事 王 一平 新 聘 中央 文史 研究 馆 馆员 孙 机 程 毅 中 也 应邀 参加 了 招待会\n",
      "Decoded:   龚龚\n",
      "loop: 259 Train cost:  296.7278195307805\n",
      "Label err rate:  0.94400024\n",
      " file 0\n",
      "Original: 喂 今年 秧田 除 稗 有 什么 好 药 一 位 腋下 夹 根 烟筒 杆 的 老农 抢先 问道\n",
      "Decoded:   的 \n",
      "loop: 279 Train cost:  294.23894942147393\n",
      "Label err rate:  0.9537178\n",
      " file 0\n",
      "Original: 去年 八月 八日 一群 初 中学 生在 该州 亨德森 县 游玩 时 抓住 二十二 只 青蛙 其中 十 一只 后腿 严重 畸形\n",
      "Decoded:   的 \n",
      "loop: 299 Train cost:  291.8445576985677\n",
      "Label err rate:  0.9497243\n",
      " file 0\n",
      "Original: 我以为 这 必能 问 住 他们 因为 他们 必 不会 在 为 怕 我 成了 老 绝 户 而 愿 每月 津贴 我 多少 钱\n",
      "Decoded:  他 的 \n",
      "loop: 319 Train cost:  289.7753082275391\n",
      "Label err rate:  0.9518155\n",
      " file 0\n",
      "Original: 十几 根 鎏 金 石雕 圆柱 承 托 着 两 层 圆形 屋顶 远 看 颇 似 我国 传统 的 重檐 圆 亭\n",
      "Decoded:   的 龚\n",
      "loop: 339 Train cost:  288.07882394229665\n",
      "Label err rate:  0.9532993\n",
      " file 0\n",
      "Original: 最后 按 专业组 业余 青年 组 业余 中老年 组 分别 产生 冠军 三人 亚军 六人 季军 九 人\n",
      "Decoded:  她 的 了\n",
      "loop: 359 Train cost:  286.69797151353623\n",
      "Label err rate:  0.9578258\n",
      " file 0\n",
      "Original: 年底 棉纺 印染厂 扭亏为盈 并 实现 利税 一千一百 万元 成为 全国 纺织 行业 扭亏 增 盈 大户\n",
      "Decoded:  这 的 了\n",
      "loop: 379 Train cost:  285.43564453125\n",
      "Label err rate:  0.9459894\n",
      " file 0\n",
      "Original: 新华社 贵阳 二月 十八 日电 贵州 高原 上 寒气 尚未 散尽 街头 黔 女 已 着 裙装 飘然 过 市 为 贵阳 带来 盈盈 春意\n",
      "Decoded:  他 的 了\n",
      "loop: 399 Train cost:  284.06055194854736\n",
      "Label err rate:  0.9407004\n",
      " file 0\n",
      "Original: 纳税人 营业额 达到 起 征 点 的 应 按 营业额 全额 计算 应 纳 税额\n",
      "Decoded:  他 的 了\n",
      "loop: 419 Train cost:  282.8134280976795\n",
      "Label err rate:  0.9624996\n",
      " file 0\n",
      "Original: 中 国和 瓦努阿图 双方 今天 在这里 就 瓦努阿图 总理 卡洛 特 访华 发表 了 新闻 公报\n",
      "Decoded:  这 了龚龚\n",
      "loop: 439 Train cost:  281.5560051311146\n",
      "Label err rate:  0.9460935\n",
      " file 0\n",
      "Original: 一个 秋天 的 夜晚 太行 山区 某 县医院 妇产科 病房里 一阵阵 凄惨 的 哭声 划破 夜空\n",
      "Decoded:  这 的 了\n",
      "loop: 459 Train cost:  280.5004913661791\n",
      "Label err rate:  0.9682826\n",
      " file 0\n",
      "Original: 新华社 维也纳 二月 二十二 日电 维也纳 欧洲 常规 裁军 第五 轮 会谈 今天 在这里 结束\n",
      "Decoded:  这 了\n",
      "loop: 479 Train cost:  279.53130871454874\n",
      "Label err rate:  0.9569772\n",
      " file 0\n",
      "Original: 阎王 保长 用 文明 棍 这里 插 一下 那里 捅 一下 一些 破布 烂片 给 他 的 文明 棍 翻 得 乱七八糟\n",
      "Decoded:  这 了龚龚\n",
      "loop: 499 Train cost:  278.6766644287109\n",
      "Label err rate:  0.9502491\n",
      " file 0\n",
      "Original: 新华社 太原 二月 五 日电 科学家 研究 发现 掌握 应用 气象 资料 能够 预防 煤矿 的 瓦斯 爆炸\n",
      "Decoded:  他 了龚龚\n",
      "loop: 519 Train cost:  278.080778327355\n",
      "Label err rate:  0.9551442\n",
      " file 0\n",
      "Original: 的确 从古到今 因 说 假话 受宠 升官 说真话 反 遭 压抑 甚至 丢 官 掉 脑袋 的 事 不胜枚举\n",
      "Decoded:  此 的 了\n",
      "loop: 539 Train cost:  277.08475392659506\n",
      "Label err rate:  0.9398043\n",
      " file 0\n",
      "Original: 我们 家 有 台 旧 音响 坏 了 以后 我妈 让 我 修 被 我 越 修 越 不成 样子 她 就 不 往回 要 了\n",
      "Decoded:  这 的 了\n",
      "loop: 559 Train cost:  276.17947812761577\n",
      "Label err rate:  0.950167\n",
      " file 0\n",
      "Original: 品 茶人 在 饮茶 前 双 手捧 碗 旋转 三 圈 赏玩 碗 上 的 花纹 质地 后 方 能 饮茶\n",
      "Decoded:  这 的 了\n",
      "loop: 579 Train cost:  275.5630579192063\n",
      "Label err rate:  0.9458395\n",
      " file 0\n",
      "Original: 各 棉纺 企业 不得 将 生产 用 棉 用以 转卖 一经 查出 即 扣 减 用 棉 企业 的 用 棉 量\n",
      "Decoded:  这 的 \n",
      "loop: 599 Train cost:  274.9935883585612\n",
      "Label err rate:  0.9642252\n",
      " file 0\n",
      "Original: 避开 对方 雷达 追踪 的 秘诀 在于 在 机体 上 喷涂 的 特殊 涂料 而 这种 特殊 涂料 是 日本 研制 的\n",
      "Decoded:  这 了龚龚\n",
      "loop: 619 Train cost:  274.4783125354398\n",
      "Label err rate:  0.9814973\n",
      " file 0\n",
      "Original: 可是 常常 有人 告诉 姚 先生 说 看见 二小姐 在 咖啡 馆里 和 王俊 业 握 着手 一坐 坐上 几个 钟头\n",
      "Decoded:  这 \n",
      "loop: 639 Train cost:  273.8727150201797\n",
      "Label err rate:  0.9452408\n",
      " file 0\n",
      "Original: 特 丽雅 女鞋 进价 一百三十四 元 加运费 二元 税金 五十八 元 及 其他费用 零售价 一百七十四 元\n",
      "Decoded:  这 的 \n",
      "loop: 659 Train cost:  273.3892654650139\n",
      "Label err rate:  0.960451\n",
      " file 0\n",
      "Original: 我 爱花 爱 自然界 的 花 更 爱 用 各种 文艺 形式 塑造 培育 的 生命 生活 的 花 友谊 的 花\n",
      "Decoded:  这  \n",
      "loop: 679 Train cost:  272.9798069224638\n",
      "Label err rate:  0.9577055\n",
      " file 0\n",
      "Original: 主要原因 是 世界 红 白藤 主产 国 印尼 禁止 红 白藤 原料 出口 导致 国际 红 白藤 原料 价格 上涨\n",
      "Decoded:  这  龚\n",
      "loop: 699 Train cost:  272.47961528233117\n",
      "Label err rate:  0.97044945\n",
      " file 0\n",
      "Original: 说到 二毛 企业 应该 像 二毛 那样 有 资金 有 技术 有 原料 保证 有 销售渠道 有 企业形象\n",
      "Decoded:  这 龚龚\n",
      "loop: 719 Train cost:  272.0943900214301\n",
      "Label err rate:  0.95694566\n",
      " file 0\n",
      "Original: 菜 做好 了 一碗 清蒸 武昌鱼 一碗 蕃茄 炒鸡蛋 一碗 榨菜 干 子 炒肉丝\n",
      "Decoded:  他  龚龚龚\n",
      "loop: 739 Train cost:  271.6840559057287\n",
      "Label err rate:  0.96009874\n",
      " file 0\n",
      "Original: 柬埔寨 副主席 乔森 潘 今天下午 到达 雅加达 时 说 雅加达 会议 能否 取得 成果 将 取决于 越南\n",
      "Decoded:  这  龚\n",
      "loop: 759 Train cost:  271.17592221310264\n",
      "Label err rate:  0.9542192\n",
      " file 0\n",
      "Original: 我 爱花 爱 自然界 的 花 更 爱 用 各种 文艺 形式 塑造 培育 的 生命 生活 的 花 友谊 的 花\n",
      "Decoded:  这  了龚\n",
      "loop: 779 Train cost:  270.71796861306217\n",
      "Label err rate:  0.9603429\n",
      " file 0\n",
      "Original: 十几 根 鎏 金 石雕 圆柱 承 托 着 两 层 圆形 屋顶 远 看 颇 似 我国 传统 的 重檐 圆 亭\n",
      "Decoded:  这  \n",
      "loop: 799 Train cost:  270.2446221733093\n",
      "Label err rate:  0.9443058\n",
      " file 0\n",
      "Original: 要 理顺 产权 关系 除了 要 理顺 政企 关系 外 首先 要 理顺 企业 资产 所有者 与 经营者 的 关系\n",
      "Decoded:  这  了龚\n",
      "loop: 819 Train cost:  269.91394013195503\n",
      "Label err rate:  0.8988031\n",
      " file 0\n",
      "Original: 小 鸳鸯 成长 很快 到了 深秋 在 北方 出生 的 小 鸳鸯 便 能 跟随 鸳鸯 大群 一起 南下 越冬 了\n",
      "Decoded:  这      \n",
      "loop: 839 Train cost:  269.49517004830494\n",
      "Label err rate:  0.93104535\n",
      " file 0\n",
      "Original: 河北省 涉县 赤 岸 村 曾 是 八路军 一二九 师 司令部 太行 军区 司令部 和 太行 区党委 的 驻地\n",
      "Decoded:  她   了龚\n",
      "loop: 859 Train cost:  269.04024482549625\n",
      "Label err rate:  0.94796133\n",
      " file 0\n",
      "Original: 我 的 肩膀 浑圆 胸前 肥嘟嘟 的 身材 又 变得 那么 矮小 尤其 是 脚下 好像 踩着 高跷 简直 要 把 脚 筋 绷 断\n",
      "Decoded:  这    了\n",
      "loop: 879 Train cost:  268.6075734745372\n",
      "Label err rate:  0.9220255\n",
      " file 0\n",
      "Original: 除 家用 个体 餐 饮用 冰柜 外 商业 用 冷藏 冷冻 陈列柜 等 专业 用 产品 也有 相当 大 的 市场\n",
      "Decoded:  他   龚\n",
      "loop: 899 Train cost:  268.2040979851617\n",
      "Label err rate:  0.95505375\n",
      " file 0\n",
      "Original: 空军 航空兵 某部 女 飞行员 王琴 刻苦 钻研 飞行 技能 最近 跨入 了 全天候 飞行员 行列\n",
      "Decoded:  他  了\n",
      "loop: 919 Train cost:  267.78324810525646\n",
      "Label err rate:  0.9203969\n",
      " file 0\n",
      "Original: 蒋 慧娟 廖 鸾凤 廖 静文 翟 文 蓉 颜 小军 潘 长 玉 潘 文兰 薛 昭 戴 丽芳\n",
      "Decoded:  另    龚\n",
      "loop: 939 Train cost:  267.45584525250376\n",
      "Label err rate:  0.9250049\n",
      " file 0\n",
      "Original: 可是 小胖子 可 别 生病 爸 的 表 娘 的 戒指 全 得 暂 入 当铺 而且 昼夜 吃 不好 睡 不安 不亚于 国难 当前\n",
      "Decoded:  他     \n",
      "loop: 959 Train cost:  267.0768729686737\n",
      "Label err rate:  0.9067253\n",
      " file 0\n",
      "Original: 南京军区 原 副 司令员 熊 应 堂 同志 因 病 于 二月 十日 在 上海 逝世 享年 八 十五岁\n",
      "Decoded:  他    了龚龚\n",
      "loop: 979 Train cost:  266.70675813324596\n",
      "Label err rate:  0.9237717\n",
      " file 0\n",
      "Original: 妈 妈的 脸色 已经 变得 象 黄土 的 颜色 一样 黄 两眼 无光 呆呆 地 看着 屋顶 一动 也 不动\n",
      "Decoded:  他   了龚\n",
      "loop: 999 Train cost:  266.4164494934082\n",
      "Label err rate:  0.9408105\n",
      " file 0\n",
      "Original: 这个 大寨 由 平 寨 东 引 羊 盘 南 贵 四个 村 组成 共 一千二百二十七 户 近 六千 名 苗族 同胞\n",
      "Decoded:  他   了龚\n",
      "loop: 1019 Train cost:  266.0740037805894\n",
      "Label err rate:  0.90767425\n",
      " file 0\n",
      "Original: 据 桑 杰 扎 巴 书记 透露 明年还要 开通 程控 直拨 电话 眼下 所需 设备 已 运 到了 八一 镇\n",
      "Decoded:  他     了\n",
      "loop: 1039 Train cost:  265.7715631925143\n",
      "Label err rate:  0.91136616\n",
      " file 0\n",
      "Original: 如此 举措 源于 杭州 娃哈哈 食品 集团公司 总经理 宗庆后 对 市场 特质 的 洞悉\n",
      "Decoded:  在    了\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 1059 Train cost:  265.4641367066581\n",
      "Label err rate:  0.905965\n",
      " file 0\n",
      "Original: 血液 循环系统 有 严重 障碍 特 别是 那些 由于 脑血管 变 窄 而 经常 头晕 头昏 的 人\n",
      "Decoded:  据     了龚\n",
      "loop: 1079 Train cost:  265.19833504005715\n",
      "Label err rate:  0.9048718\n",
      " file 0\n",
      "Original: 图 为 沈阳 旅客 张慧 泉 夫妇 怀抱 二个 可爱的 婴儿 专程 来到 济南 火车站 值班室 感谢 救命恩人\n",
      "Decoded:  这     了龚\n",
      "loop: 1099 Train cost:  264.80280077847567\n",
      "Label err rate:  0.89898264\n",
      " file 0\n",
      "Original: 此时 的 刘 玉安 奔走 于 各 大 医院 求医 问药 也 感到 一个人 力量 的 微弱 盼望 外部 的 帮助\n",
      "Decoded:  这    了龚龚\n",
      "Epoch 0/100, train_cost: 294703.297, train_ler: 0.899, time: 21925.41 sec\n",
      "epoch start: 1 total epochs=  100\n",
      "total loop  1114 in one epoch， 8 items in one loop\n",
      "loop: 19 Train cost:  249.3643898010254\n",
      "Label err rate:  0.9390823\n",
      " file 0\n",
      "Original: 而 三菱 银行 和 东京 银行 的 合并 原因 则 在于 三菱 银行 看中 了 东京 银行 的 六十 六家 海外 分行\n",
      "Decoded:  他   了\n",
      "loop: 39 Train cost:  248.83376693725586\n",
      "Label err rate:  0.9321904\n",
      " file 0\n",
      "Original: 铁路 要 从 这里 铺 通 他 不准 挖掘 老 将军 宋 承 福 带头 一 锨 首先 挖 开 了 自己 亲人 的 墓穴\n",
      "Decoded:  这    了\n",
      "loop: 59 Train cost:  248.0187525431315\n",
      "Label err rate:  0.9320938\n",
      " file 0\n",
      "Original: 杨 队长 家 的 二 娃子 发烧 到 区 上 诊所 看了 病 带回 一 包 针药 要 亦 琼 给 他 注射\n",
      "Decoded:  在   了\n",
      "loop: 79 Train cost:  246.44225578308107\n",
      "Label err rate:  0.9221873\n",
      " file 0\n",
      "Original: 从 飞 机上 往下 望去 就 如 同一个 硕大 的 玉 盆 一样 盆 底 下面 还有 缕缕 青烟 冒出\n",
      "Decoded:  这    了\n",
      "loop: 99 Train cost:  246.32305786132812\n",
      "Label err rate:  0.9027548\n",
      " file 0\n",
      "Original: 秦 华 孙 指出 为了 实现 印度洋 和平区 的 目标 需要 区域 内部 和 外部 国家 的 共同努力\n",
      "Decoded:  这     了龚\n",
      "loop: 119 Train cost:  246.5710126241048\n",
      "Label err rate:  0.90308565\n",
      " file 0\n",
      "Original: 这种 疫苗 能够 预防 养殖 的 大马哈鱼 和 鳟鱼 的 疖 病 疫苗 的 使用 可以 大大 减少 水产 养殖业 中 所 广泛 使用 的 抗菌素\n",
      "Decoded:  一      了\n",
      "loop: 139 Train cost:  245.77378649030413\n",
      "Label err rate:  0.8979107\n",
      " file 0\n",
      "Original: 猛 禽类 的 猫头鹰 秃鹫 等 脚 强壮 而 有力 趾 端 有 锐 而 钩 曲 的 爪 利于 捕杀 动物\n",
      "Decoded:  一     了龚龚\n",
      "loop: 159 Train cost:  245.74964666366577\n",
      "Label err rate:  0.9114033\n",
      " file 0\n",
      "Original: 中国 要求 在 别国 遗弃 化学武器 的 国家 应 按 公约 规定 尽快 彻底 销毁 所有 遗弃 化学武器\n",
      "Decoded:  这      了\n",
      "loop: 179 Train cost:  246.1188507080078\n",
      "Label err rate:  0.88250774\n",
      " file 0\n",
      "Original: 图 为 沈阳 旅客 张慧 泉 夫妇 怀抱 二个 可爱的 婴儿 专程 来到 济南 火车站 值班室 感谢 救命恩人\n",
      "Decoded:  他      人龚龚\n",
      "loop: 199 Train cost:  246.25262222290038\n",
      "Label err rate:  0.913375\n",
      " file 0\n",
      "Original: 在 黔 阳县 农民 全国 劳模 钦 万 有的 园艺 场 看到 四千 亩 柑桔 林 在 艳阳 下 绿 枝 摇曳 徐 部长 赞不绝口\n",
      "Decoded:  一     人龚\n",
      "loop: 219 Train cost:  246.2691560225053\n",
      "Label err rate:  0.9000134\n",
      " file 0\n",
      "Original: 要知道 急性 心肌梗塞 是 必须 住院治疗 的 到 医院 越早越好 越 晚 死亡率 越 高\n",
      "Decoded:  这     了\n",
      "loop: 239 Train cost:  246.14332230885825\n",
      "Label err rate:  0.9026629\n",
      " file 0\n",
      "Original: 新 聘 国务院 参事 王 一平 新 聘 中央 文史 研究 馆 馆员 孙 机 程 毅 中 也 应邀 参加 了 招待会\n",
      "Decoded:  在     了\n",
      "loop: 259 Train cost:  245.78891548743616\n",
      "Label err rate:  0.89501935\n",
      " file 0\n",
      "Original: 喂 今年 秧田 除 稗 有 什么 好 药 一 位 腋下 夹 根 烟筒 杆 的 老农 抢先 问道\n",
      "Decoded:  他     了\n",
      "loop: 279 Train cost:  245.71275482177734\n",
      "Label err rate:  0.87668765\n",
      " file 0\n",
      "Original: 去年 八月 八日 一群 初 中学 生在 该州 亨德森 县 游玩 时 抓住 二十二 只 青蛙 其中 十 一只 后腿 严重 畸形\n",
      "Decoded:  他       人\n",
      "loop: 299 Train cost:  245.35709416707357\n",
      "Label err rate:  0.85896397\n",
      " file 0\n",
      "Original: 我以为 这 必能 问 住 他们 因为 他们 必 不会 在 为 怕 我 成了 老 绝 户 而 愿 每月 津贴 我 多少 钱\n",
      "Decoded:  他      了龚龚\n",
      "loop: 319 Train cost:  245.18751573562622\n",
      "Label err rate:  0.90388864\n",
      " file 0\n",
      "Original: 十几 根 鎏 金 石雕 圆柱 承 托 着 两 层 圆形 屋顶 远 看 颇 似 我国 传统 的 重檐 圆 亭\n",
      "Decoded:  这     了龚\n",
      "loop: 339 Train cost:  245.0723981071921\n",
      "Label err rate:  0.8779869\n",
      " file 0\n",
      "Original: 最后 按 专业组 业余 青年 组 业余 中老年 组 分别 产生 冠军 三人 亚军 六人 季军 九 人\n",
      "Decoded:       了龚龚\n",
      "loop: 359 Train cost:  245.18965674506293\n",
      "Label err rate:  0.87084496\n",
      " file 0\n",
      "Original: 年底 棉纺 印染厂 扭亏为盈 并 实现 利税 一千一百 万元 成为 全国 纺织 行业 扭亏 增 盈 大户\n",
      "Decoded:  这      了龚龚\n",
      "loop: 379 Train cost:  245.17341581645763\n",
      "Label err rate:  0.8505066\n",
      " file 0\n",
      "Original: 新华社 贵阳 二月 十八 日电 贵州 高原 上 寒气 尚未 散尽 街头 黔 女 已 着 裙装 飘然 过 市 为 贵阳 带来 盈盈 春意\n",
      "Decoded:  他        了\n",
      "loop: 399 Train cost:  244.94503700256348\n",
      "Label err rate:  0.8735949\n",
      " file 0\n",
      "Original: 纳税人 营业额 达到 起 征 点 的 应 按 营业额 全额 计算 应 纳 税额\n",
      "Decoded:  他      的龚\n",
      "loop: 419 Train cost:  244.77530953543527\n",
      "Label err rate:  0.91456306\n",
      " file 0\n",
      "Original: 中 国和 瓦努阿图 双方 今天 在这里 就 瓦努阿图 总理 卡洛 特 访华 发表 了 新闻 公报\n",
      "Decoded:  这     了\n",
      "loop: 439 Train cost:  244.51729601079768\n",
      "Label err rate:  0.8905113\n",
      " file 0\n",
      "Original: 一个 秋天 的 夜晚 太行 山区 某 县医院 妇产科 病房里 一阵阵 凄惨 的 哭声 划破 夜空\n",
      "Decoded:  这     了龚\n",
      "loop: 459 Train cost:  244.32588564001995\n",
      "Label err rate:  0.89962965\n",
      " file 0\n",
      "Original: 新华社 维也纳 二月 二十二 日电 维也纳 欧洲 常规 裁军 第五 轮 会谈 今天 在这里 结束\n",
      "Decoded:  这    了龚\n",
      "loop: 479 Train cost:  244.16300264994302\n",
      "Label err rate:  0.87920296\n",
      " file 0\n",
      "Original: 阎王 保长 用 文明 棍 这里 插 一下 那里 捅 一下 一些 破布 烂片 给 他 的 文明 棍 翻 得 乱七八糟\n",
      "Decoded:  这      了龚\n",
      "loop: 499 Train cost:  244.06672012329102\n",
      "Label err rate:  0.8978214\n",
      " file 0\n",
      "Original: 新华社 太原 二月 五 日电 科学家 研究 发现 掌握 应用 气象 资料 能够 预防 煤矿 的 瓦斯 爆炸\n",
      "Decoded:  这     了龚\n",
      "loop: 519 Train cost:  244.15833487877478\n",
      "Label err rate:  0.9172591\n",
      " file 0\n",
      "Original: 的确 从古到今 因 说 假话 受宠 升官 说真话 反 遭 压抑 甚至 丢 官 掉 脑袋 的 事 不胜枚举\n",
      "Decoded:  这    了\n",
      "loop: 539 Train cost:  243.85545973601165\n",
      "Label err rate:  0.89773655\n",
      " file 0\n",
      "Original: 我们 家 有 台 旧 音响 坏 了 以后 我妈 让 我 修 被 我 越 修 越 不成 样子 她 就 不 往回 要 了\n",
      "Decoded:  一    了龚龚\n",
      "loop: 559 Train cost:  243.63027550833567\n",
      "Label err rate:  0.8833282\n",
      " file 0\n",
      "Original: 品 茶人 在 饮茶 前 双 手捧 碗 旋转 三 圈 赏玩 碗 上 的 花纹 质地 后 方 能 饮茶\n",
      "Decoded:  这      了龚\n",
      "loop: 579 Train cost:  243.65241886007374\n",
      "Label err rate:  0.9039818\n",
      " file 0\n",
      "Original: 各 棉纺 企业 不得 将 生产 用 棉 用以 转卖 一经 查出 即 扣 减 用 棉 企业 的 用 棉 量\n",
      "Decoded:  这     了龚\n",
      "loop: 599 Train cost:  243.63097579956056\n",
      "Label err rate:  0.90582204\n",
      " file 0\n",
      "Original: 避开 对方 雷达 追踪 的 秘诀 在于 在 机体 上 喷涂 的 特殊 涂料 而 这种 特殊 涂料 是 日本 研制 的\n",
      "Decoded:  这     了\n",
      "loop: 619 Train cost:  243.65040733583513\n",
      "Label err rate:  0.9035727\n",
      " file 0\n",
      "Original: 可是 常常 有人 告诉 姚 先生 说 看见 二小姐 在 咖啡 馆里 和 王俊 业 握 着手 一坐 坐上 几个 钟头\n",
      "Decoded:  这     了龚\n",
      "loop: 639 Train cost:  243.5700655698776\n",
      "Label err rate:  0.90679365\n",
      " file 0\n",
      "Original: 特 丽雅 女鞋 进价 一百三十四 元 加运费 二元 税金 五十八 元 及 其他费用 零售价 一百七十四 元\n",
      "Decoded:  这     了龚\n",
      "loop: 659 Train cost:  243.58234814730557\n",
      "Label err rate:  0.8570392\n",
      " file 0\n",
      "Original: 我 爱花 爱 自然界 的 花 更 爱 用 各种 文艺 形式 塑造 培育 的 生命 生活 的 花 友谊 的 花\n",
      "Decoded:  这         了\n",
      "loop: 679 Train cost:  243.59357501759249\n",
      "Label err rate:  0.88712364\n",
      " file 0\n",
      "Original: 主要原因 是 世界 红 白藤 主产 国 印尼 禁止 红 白藤 原料 出口 导致 国际 红 白藤 原料 价格 上涨\n",
      "Decoded:  这     人龚\n",
      "loop: 699 Train cost:  243.51125501360212\n",
      "Label err rate:  0.9049066\n",
      " file 0\n",
      "Original: 说到 二毛 企业 应该 像 二毛 那样 有 资金 有 技术 有 原料 保证 有 销售渠道 有 企业形象\n",
      "Decoded:  这    了龚\n",
      "loop: 719 Train cost:  243.50781633589003\n",
      "Label err rate:  0.9081829\n",
      " file 0\n",
      "Original: 菜 做好 了 一碗 清蒸 武昌鱼 一碗 蕃茄 炒鸡蛋 一碗 榨菜 干 子 炒肉丝\n",
      "Decoded:  这    了龚\n",
      "loop: 739 Train cost:  243.4699542483768\n",
      "Label err rate:  0.8709084\n",
      " file 0\n",
      "Original: 柬埔寨 副主席 乔森 潘 今天下午 到达 雅加达 时 说 雅加达 会议 能否 取得 成果 将 取决于 越南\n",
      "Decoded:  这      了龚\n",
      "loop: 759 Train cost:  243.33603150217158\n",
      "Label err rate:  0.90488636\n",
      " file 0\n",
      "Original: 我 爱花 爱 自然界 的 花 更 爱 用 各种 文艺 形式 塑造 培育 的 生命 生活 的 花 友谊 的 花\n",
      "Decoded:  这     了龚\n",
      "loop: 779 Train cost:  243.2248242500501\n",
      "Label err rate:  0.88119256\n",
      " file 0\n",
      "Original: 十几 根 鎏 金 石雕 圆柱 承 托 着 两 层 圆形 屋顶 远 看 颇 似 我国 传统 的 重檐 圆 亭\n",
      "Decoded:  这      了龚\n",
      "loop: 799 Train cost:  243.1016428565979\n",
      "Label err rate:  0.8943695\n",
      " file 0\n",
      "Original: 要 理顺 产权 关系 除了 要 理顺 政企 关系 外 首先 要 理顺 企业 资产 所有者 与 经营者 的 关系\n",
      "Decoded:  这      了龚\n",
      "loop: 819 Train cost:  243.12781576761384\n",
      "Label err rate:  0.84980893\n",
      " file 0\n",
      "Original: 小 鸳鸯 成长 很快 到了 深秋 在 北方 出生 的 小 鸳鸯 便 能 跟随 鸳鸯 大群 一起 南下 越冬 了\n",
      "Decoded:  这       了龚\n",
      "loop: 839 Train cost:  243.02992268516903\n",
      "Label err rate:  0.91027355\n",
      " file 0\n",
      "Original: 河北省 涉县 赤 岸 村 曾 是 八路军 一二九 师 司令部 太行 军区 司令部 和 太行 区党委 的 驻地\n",
      "Decoded:  这    了龚龚\n",
      "loop: 859 Train cost:  242.89309222199196\n",
      "Label err rate:  0.91074413\n",
      " file 0\n",
      "Original: 我 的 肩膀 浑圆 胸前 肥嘟嘟 的 身材 又 变得 那么 矮小 尤其 是 脚下 好像 踩着 高跷 简直 要 把 脚 筋 绷 断\n",
      "Decoded:  这     了\n",
      "loop: 879 Train cost:  242.78330296603116\n",
      "Label err rate:  0.87395334\n",
      " file 0\n",
      "Original: 除 家用 个体 餐 饮用 冰柜 外 商业 用 冷藏 冷冻 陈列柜 等 专业 用 产品 也有 相当 大 的 市场\n",
      "Decoded:  他     了龚\n",
      "loop: 899 Train cost:  242.66913896348743\n",
      "Label err rate:  0.89608014\n",
      " file 0\n",
      "Original: 空军 航空兵 某部 女 飞行员 王琴 刻苦 钻研 飞行 技能 最近 跨入 了 全天候 飞行员 行列\n",
      "Decoded:  他     了龚\n",
      "loop: 919 Train cost:  242.531800941799\n",
      "Label err rate:  0.8573549\n",
      " file 0\n",
      "Original: 蒋 慧娟 廖 鸾凤 廖 静文 翟 文 蓉 颜 小军 潘 长 玉 潘 文兰 薛 昭 戴 丽芳\n",
      "Decoded:  他      人龚龚\n",
      "loop: 939 Train cost:  242.50363295534825\n",
      "Label err rate:  0.86701035\n",
      " file 0\n",
      "Original: 可是 小胖子 可 别 生病 爸 的 表 娘 的 戒指 全 得 暂 入 当铺 而且 昼夜 吃 不好 睡 不安 不亚于 国难 当前\n",
      "Decoded:  他       了\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 959 Train cost:  242.4137996673584\n",
      "Label err rate:  0.8984556\n",
      " file 0\n",
      "Original: 南京军区 原 副 司令员 熊 应 堂 同志 因 病 于 二月 十日 在 上海 逝世 享年 八 十五岁\n",
      "Decoded:  他     了龚\n",
      "loop: 979 Train cost:  242.32591110540895\n",
      "Label err rate:  0.9312571\n",
      " file 0\n",
      "Original: 妈 妈的 脸色 已经 变得 象 黄土 的 颜色 一样 黄 两眼 无光 呆呆 地 看着 屋顶 一动 也 不动\n",
      "Decoded:  他   了龚\n",
      "loop: 999 Train cost:  242.31345471191406\n",
      "Label err rate:  0.89032745\n",
      " file 0\n",
      "Original: 这个 大寨 由 平 寨 东 引 羊 盘 南 贵 四个 村 组成 共 一千二百二十七 户 近 六千 名 苗族 同胞\n",
      "Decoded:  在      了龚\n",
      "loop: 1019 Train cost:  242.24603419584386\n",
      "Label err rate:  0.91259634\n",
      " file 0\n",
      "Original: 据 桑 杰 扎 巴 书记 透露 明年还要 开通 程控 直拨 电话 眼下 所需 设备 已 运 到了 八一 镇\n",
      "Decoded:  他    了龚\n",
      "loop: 1039 Train cost:  242.20624702160174\n",
      "Label err rate:  0.9069442\n",
      " file 0\n",
      "Original: 如此 举措 源于 杭州 娃哈哈 食品 集团公司 总经理 宗庆后 对 市场 特质 的 洞悉\n",
      "Decoded:  这   了龚\n",
      "loop: 1059 Train cost:  242.1607955356814\n",
      "Label err rate:  0.8671131\n",
      " file 0\n",
      "Original: 血液 循环系统 有 严重 障碍 特 别是 那些 由于 脑血管 变 窄 而 经常 头晕 头昏 的 人\n",
      "Decoded:  这       了龚\n",
      "loop: 1079 Train cost:  242.16384280169453\n",
      "Label err rate:  0.8996159\n",
      " file 0\n",
      "Original: 图 为 沈阳 旅客 张慧 泉 夫妇 怀抱 二个 可爱的 婴儿 专程 来到 济南 火车站 值班室 感谢 救命恩人\n",
      "Decoded:  这     了\n",
      "loop: 1099 Train cost:  242.01448361483486\n",
      "Label err rate:  0.8969753\n",
      " file 0\n",
      "Original: 此时 的 刘 玉安 奔走 于 各 大 医院 求医 问药 也 感到 一个人 力量 的 微弱 盼望 外部 的 帮助\n",
      "Decoded:  这      了\n",
      "Epoch 1/100, train_cost: 269508.083, train_ler: 0.897, time: 21840.58 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 20:26:27.763476 139946114311936 deprecation.py:323] From /home/lx/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch start: 2 total epochs=  100\n",
      "total loop  1114 in one epoch， 8 items in one loop\n",
      "loop: 19 Train cost:  238.68074111938478\n",
      "Label err rate:  0.89896065\n",
      " file 0\n",
      "Original: 而 三菱 银行 和 东京 银行 的 合并 原因 则 在于 三菱 银行 看中 了 东京 银行 的 六十 六家 海外 分行\n",
      "Decoded:  这    了龚\n",
      "loop: 39 Train cost:  238.8714698791504\n",
      "Label err rate:  0.86736625\n",
      " file 0\n",
      "Original: 铁路 要 从 这里 铺 通 他 不准 挖掘 老 将军 宋 承 福 带头 一 锨 首先 挖 开 了 自己 亲人 的 墓穴\n",
      "Decoded:  这       了龚\n",
      "loop: 59 Train cost:  238.3905024210612\n",
      "Label err rate:  0.89317703\n",
      " file 0\n",
      "Original: 杨 队长 家 的 二 娃子 发烧 到 区 上 诊所 看了 病 带回 一 包 针药 要 亦 琼 给 他 注射\n",
      "Decoded:  在    了龚龚\n",
      "loop: 79 Train cost:  236.99531898498535\n",
      "Label err rate:  0.87262666\n",
      " file 0\n",
      "Original: 从 飞 机上 往下 望去 就 如 同一个 硕大 的 玉 盆 一样 盆 底 下面 还有 缕缕 青烟 冒出\n",
      "Decoded:  他       人\n",
      "loop: 99 Train cost:  237.10666381835938\n",
      "Label err rate:  0.9036548\n",
      " file 0\n",
      "Original: 秦 华 孙 指出 为了 实现 印度洋 和平区 的 目标 需要 区域 内部 和 外部 国家 的 共同努力\n",
      "Decoded:  这     了龚\n",
      "loop: 119 Train cost:  237.44176610310873\n",
      "Label err rate:  0.9126818\n",
      " file 0\n",
      "Original: 这种 疫苗 能够 预防 养殖 的 大马哈鱼 和 鳟鱼 的 疖 病 疫苗 的 使用 可以 大大 减少 水产 养殖业 中 所 广泛 使用 的 抗菌素\n",
      "Decoded:  他    了龚\n",
      "loop: 139 Train cost:  236.7060601370675\n",
      "Label err rate:  0.890379\n",
      " file 0\n",
      "Original: 猛 禽类 的 猫头鹰 秃鹫 等 脚 强壮 而 有力 趾 端 有 锐 而 钩 曲 的 爪 利于 捕杀 动物\n",
      "Decoded:  这      了\n",
      "loop: 159 Train cost:  236.81064825057985\n",
      "Label err rate:  0.93007857\n",
      " file 0\n",
      "Original: 中国 要求 在 别国 遗弃 化学武器 的 国家 应 按 公约 规定 尽快 彻底 销毁 所有 遗弃 化学武器\n",
      "Decoded:  这     了\n",
      "loop: 179 Train cost:  237.20999755859376\n",
      "Label err rate:  0.85516757\n",
      " file 0\n",
      "Original: 图 为 沈阳 旅客 张慧 泉 夫妇 怀抱 二个 可爱的 婴儿 专程 来到 济南 火车站 值班室 感谢 救命恩人\n",
      "Decoded:  这       人龚\n",
      "loop: 199 Train cost:  237.36020385742188\n",
      "Label err rate:  0.9070173\n",
      " file 0\n",
      "Original: 在 黔 阳县 农民 全国 劳模 钦 万 有的 园艺 场 看到 四千 亩 柑桔 林 在 艳阳 下 绿 枝 摇曳 徐 部长 赞不绝口\n",
      "Decoded:  这     人\n",
      "loop: 219 Train cost:  237.40864098288796\n",
      "Label err rate:  0.88532364\n",
      " file 0\n",
      "Original: 要知道 急性 心肌梗塞 是 必须 住院治疗 的 到 医院 越早越好 越 晚 死亡率 越 高\n",
      "Decoded:  在      了\n",
      "loop: 239 Train cost:  237.38925387064617\n",
      "Label err rate:  0.86231405\n",
      " file 0\n",
      "Original: 新 聘 国务院 参事 王 一平 新 聘 中央 文史 研究 馆 馆员 孙 机 程 毅 中 也 应邀 参加 了 招待会\n",
      "Decoded:  在       了龚\n",
      "loop: 259 Train cost:  237.08799162644607\n",
      "Label err rate:  0.87138647\n",
      " file 0\n",
      "Original: 喂 今年 秧田 除 稗 有 什么 好 药 一 位 腋下 夹 根 烟筒 杆 的 老农 抢先 问道\n",
      "Decoded:  一 人      了\n",
      "loop: 279 Train cost:  237.0624014173235\n"
     ]
    }
   ],
   "source": [
    "# create the session\n",
    "sess = tf.Session()\n",
    "# 没有模型的话，就重新初始化\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "kpt = tf.train.latest_checkpoint(savedir)\n",
    "print(\"kpt:\",kpt)\n",
    "startepo= 0\n",
    "if kpt!=None:\n",
    "    saver.restore(sess, kpt) \n",
    "    ind = kpt.find(\"-\")\n",
    "    startepo = int(kpt[ind+1:])\n",
    "    print(startepo)\n",
    "\n",
    "# 准备运行训练步骤\n",
    "section = '\\n{0:=^40}\\n'\n",
    "print(section.format('Run training epoch'))\n",
    "\n",
    "train_start = time.time()\n",
    "for epoch in range(epochs):#样本集迭代次数\n",
    "    epoch_start = time.time()\n",
    "    if epoch<startepo:\n",
    "        continue\n",
    "   \n",
    "    print(\"epoch start:\",epoch,\"total epochs= \",epochs)\n",
    "#######################run batch####\n",
    "    n_batches_per_epoch = int(np.ceil(len(labels) / batch_size))\n",
    "    print(\"total loop \",n_batches_per_epoch,\"in one epoch，\",batch_size,\"items in one loop\") \n",
    "    \n",
    "    train_cost = 0\n",
    "    train_ler = 0\n",
    "    next_idx =0\n",
    "    \n",
    "    for batch in range(n_batches_per_epoch):#一次batch_size，取多少次\n",
    "        #取数据\n",
    "        next_idx,source,source_lengths,sparse_labels = \\\n",
    "            next_batch(labels,next_idx ,batch_size)\n",
    "        feed = {input_tensor: source, targets: sparse_labels,seq_length: source_lengths,keep_dropout:keep_dropout_rate}\n",
    "        \n",
    "        #计算 avg_loss optimizer ;\n",
    "        batch_cost, _ = sess.run([avg_loss, optimizer],  feed_dict=feed )\n",
    "        train_cost += batch_cost \n",
    "             \n",
    "        if (batch +1)%20 == 0:\n",
    "            print('loop:',batch, 'Train cost: ', train_cost/(batch+1))\n",
    "            feed2 = {input_tensor: source, targets: sparse_labels,seq_length: source_lengths,keep_dropout:1.0}\n",
    "\n",
    "            d,train_ler = sess.run([decoded[0],ler], feed_dict=feed2)\n",
    "            dense_decoded = tf.sparse_tensor_to_dense( d, default_value=-1).eval(session=sess)\n",
    "            dense_labels = sparse_tuple_to_texts_ch(sparse_labels,words)\n",
    "            \n",
    "            counter =0\n",
    "            print('Label err rate: ', train_ler)\n",
    "            for orig, decoded_arr in zip(dense_labels, dense_decoded):\n",
    "                # convert to strings\n",
    "                decoded_str = ndarray_to_text_ch(decoded_arr,words)\n",
    "                print(' file {}'.format( counter))\n",
    "                print('Original: {}'.format(orig))\n",
    "                print('Decoded:  {}'.format(decoded_str))\n",
    "                counter=counter+1\n",
    "                break\n",
    "            \n",
    "        \n",
    "    epoch_duration = time.time() - epoch_start\n",
    "    \n",
    "    log = 'Epoch {}/{}, train_cost: {:.3f}, train_ler: {:.3f}, time: {:.2f} sec'\n",
    "    print(log.format(epoch ,epochs, train_cost,train_ler,epoch_duration))\n",
    "    saver.save(sess, savedir+\"yuyinch.cpkt\", global_step=epoch)\n",
    "    \n",
    "        \n",
    "    \n",
    "train_duration = time.time() - train_start\n",
    "print('Training complete, total duration: {:.2f} min'.format(train_duration / 60))\n",
    "\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
